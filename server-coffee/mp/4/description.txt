### MP4: Reduction

*MP4  is an optional assignment. You will not be penalized for not doing the assignment. *

#### Objective

Implement a kernel the performs reduction of a 1D list.
The reduction should give the sum of the list.
You should implement the improved kernel in Lecture 5.7. Your kernel should be able to handle input lists of arbitrary length.
However, for simplicity, you can assume that the input list will be at most 2048 * 65,535 elements so that it can be handled by only one kernel launch.
The boundary condition can be handeled by filling "identify value (0 for sum)"" into the shared memory of the last block when the length is not a multiple of the thread block size.
Further assume that the reduction sums of each section generated by individual blocks will be summed up by the CPU. Prerequisites

#### Prerequisites

Before starting this lab, make sure that:

* You have completed MP3

* You can have completed lecture 5.5 through 5.7

#### Instruction

Edit the code in the code tab to perform the following:

- allocate device memory
- copy host memory to device
- initialize thread block and kernel grid dimensions
- invoke CUDA kernel
- copy results from device to host
- deallocate device memory
- implement the improved reduction routine
- use shared memory to  reduce the number of global accesses, handle the boundary conditions in when loading input list elements into the shared memory
- implement a CPU loop to perform final reduction based on the sums of sections generated by the thread blocks

Instructions about where to place each part of the code is
demarcated by the `//@@` comment lines.

#### Grading

You will be grading based on the following rubric:

- Compilation (no warning): 15%
- Run time (with respect to other students): 10%
- Correctness (generates correct sum and deals with boundary conditions correctly): 75%

If we cannot compile your program, then you will get 0 points.
Note that we only grade the last program submitted and do not accept
programs beyond the deadline.

For the run time grade, we will take the average time of all the final submissions. If your time is no more than 10% above the average time, then you will get full marks. Otherwise, you will lose points.

Note that the datasets that we test against are not the same as the ones provided, so make sure to code the algorithm for correctness on general datasets not just the ones provided.


#### Suggestions

* Develop your application incrementally

* Do not wait until the last minute to attempt the lab

* Check for CUDA errors, here is some example `wbCheck` that you can use (included in the template code):
    

         #define wbCheck(stmt) do {                                 \
                 cudaError_t err = stmt;                            \
                 if (err != cudaSuccess) {                          \
                     wbLog(ERROR, "Failed to run stmt ", #stmt);    \
                     return -1;                                     \
                 }                                                  \
             } while(0)

using this in your code would look like `wbCheck(cudaMalloc(...))`

* Make sure that your algorithm handles boundary conditions where the length of the input list may not be a multiple of the block/tile size

* Make sure that your algorithm handles rectangular matrices, not just square matrices as shown in the slides.

* Do not modify the template code written -- only insert code where the `//@@` demarcation is placed

* Make sure that you test your program using all the datasets provided (the datasets can be selected using the dropdown next to the submission button)

* Even though you can submit multiple times, only your last submission is graded

